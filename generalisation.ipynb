{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from sklearn.metrics import f1_score\n",
    "import random\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "\n",
    "NOMBRE =2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the tokenizer from /home/daril_kw/data/tokenizer_final\n",
    "tokenizer = BertTokenizer.from_pretrained('/home/daril_kw/data/tokenizer_final')\n",
    "#load the dataset from home/daril_kw/data/data_with_time_info_ok.json\n",
    "data_format = json.load(open('/home/daril_kw/data/data_with_time_info_ok.json', 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/daril_kw/data//home/daril_kw/data/data_with_time_info_ok.json', 'r') as openfile:\n",
    "    json_loaded = json.load(openfile)\n",
    "    \n",
    "data_format = pd.DataFrame(data=json_loaded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contexte : le même qu'on soit dans cas train/validation ou test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_format['HOUR']=data_format['HOUR'].apply(lambda x: ' '+x)\n",
    "data_format['WEEK']=data_format['WEEK'].apply(lambda x: ' '+x)\n",
    "data_format['CALL_TYPE']=data_format['CALL_TYPE'].apply(lambda x: ' '+x)\n",
    "data_format['TAXI_ID']=data_format['TAXI_ID'].apply(lambda x: ' '+x)\n",
    "data_format['DAY']=data_format['DAY'].apply(lambda x: ' '+x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# la colonne CONTEXT_INPUT sera la concaténation du jour de la semaine, de l'heure et de la semaien de l'année pui de la colonne CALL_TYPE, de la colonne TAXI_ID, d'un espace et du dernier token de la colonne Tokenization\n",
    "data_format['CONTEXT_INPUT'] =data_format['Tokenization_2'].apply(lambda x: x[-1]) + data_format['DAY'] + data_format['HOUR'] + data_format['WEEK'] + data_format['CALL_TYPE'] + data_format['TAXI_ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#on récupère le nombre d'informations dans la colonne CONTEXT_INPUT\n",
    "#Comme cette colonne contiient les informations en string séparé par un espace, on récupère la liste correspondante puis on compte le nombre d'éléments de cette liste\n",
    "len_context_info = len(data_format['CONTEXT_INPUT'][0].split(' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Séparation du dataframe   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we separate the dataframe into train and test \n",
    "data_train, data_test = train_test_split(data_format, test_size=0.2, random_state=2023)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gestion du dataframe de training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we define a function that take a dataframe and five other parameters\n",
    "#and that decides which token to put into the target\n",
    "#the last token before the target is the last token taken in DEB_TRAJ\n",
    "#three of the parameters of the function will determine the probability to choose the target between the  last 30% and 60% / in the last 10% and 30%/ in the 10% last tokens\n",
    "#one parameter is for the proportion we will take the very last token as target\n",
    "#the last one is for the proportion we will take the [SEP] token (after the very last token) as target\n",
    "\n",
    "def prepare(dataframe, sixty_percent, thirty_percent, ten_percent, last_prob, sep_prob ):\n",
    "    #sixty_percent is the proportion of the time we will take the target in the 60% last tokens\n",
    "    #thirty_percent is the proportion of the time we will take the target in the 30% last tokens\n",
    "    #ten_percent is the proportion of the time we will take the target in the 10% last tokens\n",
    "    #last_prob is the proportion of the time we will take the very last token as target\n",
    "    #sep_prob is the proportion of the time we will take the [SEP] token (after the very last token) as target\n",
    "    #the sum of the five parameters must be equal to 1, we verify it\n",
    "    #if it is not the case, we raise an error\n",
    "    if sixty_percent+thirty_percent+ten_percent+last_prob+sep_prob!=1:\n",
    "        raise ValueError('The sum of the five parameters must be equal to 1')\n",
    "    #random.random() return a random float number between 0 and 1\n",
    "    list_random=[random.random() for i in range(len(dataframe))]\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "data_train['DEB_TRAJ']=data_train['Tokenization_2'].apply(lambda x: x[:-NOMBRE])\n",
    "data_train['DEB_TRAJ']=data_train['DEB_TRAJ'].apply(lambda x: x[-(512-len_context_info-2):] if len(x)>512-len_context_info-2 else x)\n",
    "#then we keep the column in form of a string\n",
    "data_train['DEB_TRAJ']=data_train['DEB_TRAJ'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "\n",
    "data_train['TARGET']=data_train['Tokenization_2'].apply(lambda x: x[-NOMBRE:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#on enlève les colonnes inutiles\n",
    "if 'Tokenization' in data_train.columns:\n",
    "    data_train.drop(['Tokenization'],axis=1,inplace=True)\n",
    "if 'CALL_TYPE' in data_train.columns:\n",
    "    data_train.drop(['CALL_TYPE'],axis=1,inplace=True)\n",
    "if 'TAXI_ID' in data_train.columns:\n",
    "    data_train.drop(['TAXI_ID'],axis=1,inplace=True)\n",
    "if 'DAY' in data_train.columns:\n",
    "    data_train.drop(['DAY'],axis=1,inplace=True)\n",
    "if 'HOUR' in data_train.columns:\n",
    "    data_train.drop(['HOUR'],axis=1,inplace=True)\n",
    "if 'WEEK' in data_train.columns:\n",
    "    data_train.drop(['WEEK'],axis=1,inplace=True)\n",
    "if 'Nb_points_token' in data_train.columns:\n",
    "    data_train.drop(['Nb_points_token'],axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#on sauvegarde le fichier au format json\n",
    "data_train.to_json('/home/daril_kw/data/data_train.json',orient='records')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concaténation, padding et ajout tokens spéciaux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_inputs=data_train.CONTEXT_INPUT.values\n",
    "traj_inputs=data_train.DEB_TRAJ.values\n",
    "targets=data_train.TARGET.values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gestion de l'entrée : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = []\n",
    "full_inputs = []\n",
    "attention_masks = []\n",
    "for i in tqdm(range(len(c_inputs))):\n",
    "    #no truncation is needed because we managed it before\n",
    "\n",
    "    #we concatenate the context input and the trajectory input adding manually the CLS token and the SEP token\n",
    "    full_input = '[CLS] ' + c_inputs[i] + ' ' + traj_inputs[i] + ' [SEP]'\n",
    "    full_inputs.append(full_input)\n",
    "    #encoded_c_input=tokenizer.encode(c_inputs[i], add_special_tokens=False)\n",
    "    #encoded_traj_input=tokenizer.encode(traj_inputs[i], add_special_tokens=False)\n",
    "    #we add manually the CLS token and the SEP token when we concatenate the two inputs\n",
    "    #encoded_full_input=[101] + encoded_c_input + encoded_traj_input + [102]\n",
    "    #the[101] token is the CLS token and the [102] token is the SEP token\n",
    "\n",
    "    encoded_full_input=tokenizer.encode(full_input, add_special_tokens=False)\n",
    "    #we pad the input to the maximum length of 512\n",
    "    encoded_full_input=encoded_full_input + [0]*(512-len(encoded_full_input))\n",
    "    input_ids.append(encoded_full_input)\n",
    "    #we create the attention mask\n",
    "    att_mask = [float(i>0) for i in encoded_full_input]\n",
    "    attention_masks.append(att_mask)\n",
    "    #the attention mask is a list of 0 and 1, 0 for the padded tokens and 1 for the other tokens\n",
    "    #the float(i>0) is 0 if i=0 (ie if the token is a padded token) and 1 if i>0 (ie if the token is not a padded token)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gestion des targets :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_dict={}\n",
    "for i in range(len(targets)):\n",
    "    if targets[i] not in targets_dict:\n",
    "        targets_dict[targets[i]]=len(targets_dict)\n",
    "\n",
    "targets_input=[targets_dict[targets[i]] for i in range(len(targets))]\n",
    "\n",
    "#le dictionnaire est sauvegardé au format json\n",
    "with open('/home/daril_kw/data/targets_dict.json', 'w') as fp:\n",
    "    json.dump(targets_dict, fp)\n",
    "\n",
    "# le dictionnaire s'utilisera comme suit : \n",
    "\n",
    "# soit un entier 'target' non compris entre 0 et len(targets_dict)-1\n",
    "# on récupère la valeur correspondante dans le dictionnaire avec targets_dict[target]\n",
    "# inversement, si on a un entier 'target_encoded' compris entre 0 et len(targets_dict)-1\n",
    "# on récupère la clé correspondante dans le dictionnaire avec list(targets_dict.keys())[target_encoded]\n",
    "\n",
    "#la liste targets_input contient les targets du dataset encodées avec le dictionnaire targets_dict ie dans leur nouvel espace\n",
    "#save in pickle the targets\n",
    "\n",
    "with open('/home/daril_kw/data/targets_input.pkl', 'wb') as f:\n",
    "    pickle.dump(targets_input, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"data_train['DEB_TRAJ']=data_train['Tokenization_2'].apply(lambda x: x[:-NOMBRE])\n",
    "data_train['DEB_TRAJ']=data_train['DEB_TRAJ'].apply(lambda x: x[-(512-len_context_info-2):] if len(x)>512-len_context_info-2 else x)\n",
    "#then we keep the column in form of a string\n",
    "data_train['DEB_TRAJ']=data_train['DEB_TRAJ'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "\n",
    "data_train['TARGET']=data_train['Tokenization_2'].apply(lambda x: x[-NOMBRE:-1]) pour test\"\"\" "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
