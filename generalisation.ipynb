{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from sklearn.metrics import f1_score\n",
    "import random\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "\n",
    "NOMBRE =2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the tokenizer from /home/daril_kw/data/tokenizer_final\n",
    "tokenizer = BertTokenizer.from_pretrained('/home/daril_kw/data/tokenizer_final')\n",
    "#load the dataset from home/daril_kw/data/data_with_time_info_ok.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/daril_kw/data//home/daril_kw/data/data_with_time_info_ok.json', 'r') as openfile:\n",
    "    json_loaded = json.load(openfile)\n",
    "    \n",
    "data_format = pd.DataFrame(data=json_loaded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contexte : le même qu'on soit dans cas train/validation ou test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_format['HOUR']=data_format['HOUR'].apply(lambda x: ' '+x)\n",
    "data_format['WEEK']=data_format['WEEK'].apply(lambda x: ' '+x)\n",
    "data_format['CALL_TYPE']=data_format['CALL_TYPE'].apply(lambda x: ' '+x)\n",
    "data_format['TAXI_ID']=data_format['TAXI_ID'].apply(lambda x: ' '+x)\n",
    "data_format['DAY']=data_format['DAY'].apply(lambda x: ' '+x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# la colonne CONTEXT_INPUT sera la concaténation du jour de la semaine, de l'heure et de la semaien de l'année pui de la colonne CALL_TYPE, de la colonne TAXI_ID, d'un espace et du dernier token de la colonne Tokenization\n",
    "data_format['CONTEXT_INPUT'] =data_format['Tokenization_2'].apply(lambda x: x[-1]) + data_format['DAY'] + data_format['HOUR'] + data_format['WEEK'] + data_format['CALL_TYPE'] + data_format['TAXI_ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#on récupère le nombre d'informations dans la colonne CONTEXT_INPUT\n",
    "#Comme cette colonne contiient les informations en string séparé par un espace, on récupère la liste correspondante puis on compte le nombre d'éléments de cette liste\n",
    "len_context_info = len(data_format['CONTEXT_INPUT'][0].split(' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Séparation du dataframe   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we separate the dataframe into train and test \n",
    "data_train, data_test = train_test_split(data_format, test_size=0.2, random_state=2023)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gestion du dataframe de training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we define a function that take a dataframe and five other parameters\n",
    "#and that decides which token to put into the target\n",
    "#the last token before the target is the last token taken in DEB_TRAJ\n",
    "#three of the parameters of the function will determine the probability to choose the target between the  last 30% and 60% / in the last 10% and 30%/ in the 10% last tokens\n",
    "#one parameter is for the proportion we will take the very last token as target\n",
    "#the last one is for the proportion we will take the [SEP] token (after the very last token) as target\n",
    "\n",
    "def prepare_train(dataframe, sixty_percent=0.4, thirty_percent=0.25, ten_percent=0.15, last_prob=0.1, sep_prob=0.1 ):\n",
    "    #sixty_percent is the proportion of the time we will take the target between the 60% last tokens and the 30% last tokens\n",
    "    #thirty_percent is the proportion of the time we will take the target between the 30% last tokens and the 10% last tokens\n",
    "    #ten_percent is the proportion of the time we will take the target in the 10% last tokens\n",
    "    #last_prob is the proportion of the time we will take the very last token as target\n",
    "    #sep_prob is the proportion of the time we will take the [SEP] token (after the very last token) as target\n",
    "    #the sum of the five parameters must be equal to 1, we verify it\n",
    "    #if it is not the case, we raise an error\n",
    "    if sixty_percent+thirty_percent+ten_percent+last_prob+sep_prob!=1:\n",
    "        raise ValueError('The sum of the five parameters must be equal to 1')\n",
    "    #random.random() return a random float number between 0 and 1\n",
    "    \n",
    "    #we create a list of the rows that will fall into the 5 categories (between 60% and 30%, between 30% and 10%, in the 10%, the very last token and the [SEP] token)\n",
    "    list_sixty = []\n",
    "    list_thirty = []\n",
    "    list_ten = []\n",
    "    list_last = []\n",
    "    list_sep = []\n",
    "    #we create a seed to be able to reproduce the results\n",
    "    random.seed(2023)\n",
    "    #wealculate the number of rows that will fall into each category\n",
    "    number_sixty = int(len(dataframe)*sixty_percent)\n",
    "    number_thirty = int(len(dataframe)*thirty_percent)\n",
    "    number_ten = int(len(dataframe)*ten_percent)\n",
    "    number_last = int(len(dataframe)*last_prob)\n",
    "    number_sep = int(len(dataframe)*sep_prob)\n",
    "\n",
    "    #due to the conversion of int, we may have a number of rows that is not equal to the number of rows of the dataframe\n",
    "    # if the sum of number_sixty, number_thirty, number_ten, number_last and number_sep is not equal to the number of rows of the dataframe\n",
    "    #we add the missing rows to one of the categories randomly\n",
    "    while number_sixty+number_thirty+number_ten+number_last+number_sep!=len(dataframe):\n",
    "        #we choose randomly one of the categories\n",
    "        category = random.choice(['sixty', 'thirty', 'ten', 'last', 'sep'])\n",
    "        #we add one row to the category\n",
    "        if category == 'sixty':\n",
    "            number_sixty+=1\n",
    "        elif category == 'thirty':\n",
    "            number_thirty+=1\n",
    "        elif category == 'ten':\n",
    "            number_ten+=1\n",
    "        elif category == 'last':\n",
    "            number_last+=1\n",
    "        else :\n",
    "            number_sep+=1\n",
    "\n",
    "\n",
    "\n",
    "    #we create a list of the indexes of the rows of the dataframe\n",
    "    list_index = list(range(len(dataframe)))\n",
    "    #we shuffle the list of indexes\n",
    "    random.shuffle(list_index)\n",
    "    #we create a list of the indexes of the rows that will fall into each category\n",
    "    list_index_sixty = list_index[:number_sixty]\n",
    "    list_index_thirty = list_index[number_sixty:number_sixty+number_thirty]\n",
    "    list_index_ten = list_index[number_sixty+number_thirty:number_sixty+number_thirty+number_ten]\n",
    "    list_index_last = list_index[number_sixty+number_thirty+number_ten:number_sixty+number_thirty+number_ten+number_last]\n",
    "    list_index_sep = list_index[number_sixty+number_thirty+number_ten+number_last:number_sixty+number_thirty+number_ten+number_last+number_sep]\n",
    "    \n",
    "    #we create 5 dataframes of the rows that will fall into the 5 categories (between 60% and 30%, between 30% and 10%, in the 10%, the very last token and the [SEP] token)\n",
    "    #declare the dataframes\n",
    "    dataframe_sixty = pd.DataFrame()\n",
    "    dataframe_thirty = pd.DataFrame()\n",
    "    dataframe_ten = pd.DataFrame()\n",
    "    dataframe_last = pd.DataFrame()\n",
    "    dataframe_sep = pd.DataFrame()\n",
    "    #the columns of the dataframes will be the same as the columns of the dataframe in parameter\n",
    "    dataframe_sixty = dataframe_sixty.reindex(columns=dataframe.columns)\n",
    "    dataframe_thirty = dataframe_thirty.reindex(columns=dataframe.columns)\n",
    "    dataframe_ten = dataframe_ten.reindex(columns=dataframe.columns)\n",
    "    dataframe_last = dataframe_last.reindex(columns=dataframe.columns)\n",
    "    dataframe_sep = dataframe_sep.reindex(columns=dataframe.columns)\n",
    "    #fill the dataframes \n",
    "    #the append does not exist for dataframes, we use the concat function \n",
    "    dataframe_sixty = pd.concat([dataframe_sixty, dataframe.iloc[list_index_sixty]])\n",
    "    dataframe_thirty = pd.concat([dataframe_thirty, dataframe.iloc[list_index_thirty]])\n",
    "    dataframe_ten = pd.concat([dataframe_ten, dataframe.iloc[list_index_ten]])\n",
    "    dataframe_last = pd.concat([dataframe_last, dataframe.iloc[list_index_last]])\n",
    "    dataframe_sep = pd.concat([dataframe_sep, dataframe.iloc[list_index_sep]])\n",
    "\n",
    "\n",
    "    #for each dataframe, we take the tokenization_2 column and we choose the target according to the category\n",
    "\n",
    "    #to avoid the caveat of the dataframe, we use the copy function\n",
    "    dataframe_sixty = dataframe_sixty.copy()\n",
    "    dataframe_thirty = dataframe_thirty.copy()\n",
    "    dataframe_ten = dataframe_ten.copy()\n",
    "    dataframe_last = dataframe_last.copy()\n",
    "    dataframe_sep = dataframe_sep.copy()\n",
    "\n",
    "\n",
    "\n",
    "    #for the dataframe_sixty\n",
    "\n",
    "    # we create a list of targets\n",
    "    list_target_full_data = [0 for i in range(len(dataframe))]\n",
    "    list_target_sixty_data = [0 for i in range(len(dataframe_sixty))]\n",
    "    list_deb_traj_full_data = [0 for i in range(len(dataframe))]\n",
    "    list_deb_traj_sixty_data = [0 for i in range(len(dataframe_sixty))]\n",
    "    for i in range(len(dataframe_sixty)):\n",
    "        \n",
    "        #we take the tokenization_2 column\n",
    "        tokenization_2 = dataframe_sixty.iloc[i]['Tokenization_2']\n",
    "        #we take the token that is between the 60% and the 30% last tokens\n",
    "        tokenization_2 = tokenization_2[-int(len(tokenization_2)*0.6):-int(len(tokenization_2)*0.3)]\n",
    "        #we take a random index in the list\n",
    "        index = random.randint(0,len(tokenization_2)-1)\n",
    "        list_target_full_data[list_index_sixty[i]] = tokenization_2[index]\n",
    "        list_target_sixty_data[i] = tokenization_2[index]\n",
    "        #we put in the column DEB_TRAJ the tokens that are before the target\n",
    "        list_deb_traj_full_data[list_index_sixty[i]] = dataframe.iloc[list_index_sixty[i]]['Tokenization_2'][:-int(len(tokenization_2)*0.6)+index]\n",
    "        list_deb_traj_sixty_data[i] = dataframe_sixty.iloc[i]['Tokenization_2'][:index]\n",
    "        \"\"\"\n",
    "        tokenization_2 = dataframe_sixty.iloc[i]['Tokenization_2']\n",
    "        #we take the token that is between the 60% and the 30% last tokens\n",
    "        tokenization_2 = tokenization_2[-int(len(tokenization_2)*0.6):-int(len(tokenization_2)*0.3)]\n",
    "        #we take a random index in the list\n",
    "        index = random.randint(0,len(tokenization_2)-1)\n",
    "        dataframe_sixty.iloc[i]['TARGET'] = tokenization_2[index]\n",
    "        dataframe.iloc[list_index_sixty[i]]['TARGET'] = tokenization_2[index]\n",
    "        #we put in the column DEB_TRAJ the tokens that are before the target\n",
    "        dataframe_sixty.iloc[i]['DEB_TRAJ'] =  dataframe_sixty.iloc[i]['Tokenization_2'][:-int(len(tokenization_2)*0.6)+index]\n",
    "        dataframe.iloc[list_index_sixty[i]]['DEB_TRAJ'] =  dataframe.iloc[list_index_sixty[i]]['Tokenization_2'][:-int(len(tokenization_2)*0.6)+index]\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        #we put the token in the target column\n",
    "\n",
    "\n",
    "\n",
    "        dataframe_sixty.iloc[i]['TARGET'] = tokenization_2[index]\n",
    "        dataframe.iloc[list_index_sixty[i]]['TARGET'] = tokenization_2[index]\n",
    "        #we put in the column DEB_TRAJ the tokens that are before the target\n",
    "        dataframe_sixty.iloc[i]['DEB_TRAJ'] =  dataframe_sixty.iloc[i]['Tokenization_2'][:-int(len(tokenization_2)*0.6)+index]\n",
    "        dataframe.iloc[list_index_sixty[i]]['DEB_TRAJ'] =  dataframe.iloc[list_index_sixty[i]]['Tokenization_2'][:-int(len(tokenization_2)*0.6)+index]\n",
    "        \"\"\"\n",
    "    dataframe_sixty['TARGET'] = list_target_sixty_data\n",
    "    \n",
    "    dataframe_sixty['DEB_TRAJ'] = list_deb_traj_sixty_data\n",
    "   \n",
    "\n",
    "\n",
    "    #for the dataframe_thirty\n",
    "    for i in range(len(dataframe_thirty)):\n",
    "        tokenization_2 = dataframe_thirty.iloc[i]['Tokenization_2']\n",
    "        tokenization_2 = tokenization_2[-int(len(tokenization_2)*0.3):-int(len(tokenization_2)*0.1)]\n",
    "        index = random.randint(0,len(tokenization_2)-1)\n",
    "        dataframe_thirty.iloc[i]['TARGET'] = tokenization_2[index]\n",
    "        dataframe.iloc[list_index_thirty[i]]['TARGET'] = tokenization_2[index]\n",
    "        dataframe_thirty.iloc[i]['DEB_TRAJ'] =  dataframe_thirty.iloc[i]['Tokenization_2'][:-int(len(tokenization_2)*0.3)+index]\n",
    "        dataframe.iloc[list_index_thirty[i]]['DEB_TRAJ'] =  dataframe.iloc[list_index_thirty[i]]['Tokenization_2'][:-int(len(tokenization_2)*0.3)+index]\n",
    "\n",
    "    #for the dataframe_ten\n",
    "    for i in range(len(dataframe_ten)):\n",
    "        tokenization_2 = dataframe_ten.iloc[i]['Tokenization_2']\n",
    "        tokenization_2 = tokenization_2[-int(len(tokenization_2)*0.1):-1]\n",
    "        index = random.randint(0,len(tokenization_2)-1)\n",
    "        dataframe_ten.iloc[i]['TARGET'] = tokenization_2[index]\n",
    "        dataframe.iloc[list_index_ten[i]]['TARGET'] = tokenization_2[index]\n",
    "        dataframe_ten.iloc[i]['DEB_TRAJ'] =  dataframe_ten.iloc[i]['Tokenization_2'][:-int(len(tokenization_2)*0.1)+index]\n",
    "        dataframe.iloc[list_index_ten[i]]['DEB_TRAJ'] =  dataframe.iloc[list_index_ten[i]]['Tokenization_2'][:-int(len(tokenization_2)*0.1)+index]\n",
    "\n",
    "    #for the dataframe_last\n",
    "    for i in range(len(dataframe_last)):\n",
    "        tokenization_2 = dataframe_last.iloc[i]['Tokenization_2']\n",
    "        tokenization_2 = tokenization_2[-1]\n",
    "        dataframe_last.iloc[i]['TARGET'] = tokenization_2\n",
    "        dataframe.iloc[list_index_last[i]]['TARGET'] = tokenization_2\n",
    "        #we do not include the last token in the DEB_TRAJ\n",
    "        dataframe_last.iloc[i]['DEB_TRAJ'] =  dataframe_last.iloc[i]['Tokenization_2'][:-1]\n",
    "        dataframe.iloc[list_index_last[i]]['DEB_TRAJ'] =  dataframe.iloc[list_index_last[i]]['Tokenization_2'][:-1]\n",
    "\n",
    "    #for the dataframe_sep\n",
    "    for i in range(len(dataframe_sep)):\n",
    "        dataframe_sep.iloc[i]['TARGET'] = '[SEP]'\n",
    "        dataframe.iloc[list_index_sep[i]]['TARGET'] = '[SEP]'\n",
    "        #the the DEB_TRAJ is the same as the tokenization_2\n",
    "        dataframe_sep.iloc[i]['DEB_TRAJ'] =  dataframe_sep.iloc[i]['Tokenization_2']\n",
    "        dataframe.iloc[list_index_sep[i]]['DEB_TRAJ'] =  dataframe.iloc[list_index_sep[i]]['Tokenization_2']\n",
    "\n",
    "    dataframe['TARGET'] = list_target_full_data\n",
    "\n",
    "    #we keep only the 512-len_context_info-2 last tokens in the DEB_TRAJ\n",
    "    dataframe['DEB_TRAJ']=dataframe['DEB_TRAJ'].apply(lambda x: x[-(512-len_context_info-2):] if len(x)>512-len_context_info-2 else x)\n",
    "    #then we keep the column in form of a string\n",
    "    dataframe['DEB_TRAJ']=dataframe['DEB_TRAJ'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "#data_train['DEB_TRAJ']=data_train['Tokenization_2'].apply(lambda x: x[:-NOMBRE])\n",
    "#data_train['DEB_TRAJ']=data_train['DEB_TRAJ'].apply(lambda x: x[-(512-len_context_info-2):] if len(x)>512-len_context_info-2 else x)\n",
    "#then we keep the column in form of a string\n",
    "#data_train['DEB_TRAJ']=data_train['DEB_TRAJ'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "\n",
    "#data_train['TARGET']=data_train['Tokenization_2'].apply(lambda x: x[-NOMBRE:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_train(data_train, 0.4, 0.25,0.15,0.1,0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#on enlève les colonnes inutiles\n",
    "if 'Tokenization' in data_train.columns:\n",
    "    data_train.drop(['Tokenization'],axis=1,inplace=True)\n",
    "if 'CALL_TYPE' in data_train.columns:\n",
    "    data_train.drop(['CALL_TYPE'],axis=1,inplace=True)\n",
    "if 'TAXI_ID' in data_train.columns:\n",
    "    data_train.drop(['TAXI_ID'],axis=1,inplace=True)\n",
    "if 'DAY' in data_train.columns:\n",
    "    data_train.drop(['DAY'],axis=1,inplace=True)\n",
    "if 'HOUR' in data_train.columns:\n",
    "    data_train.drop(['HOUR'],axis=1,inplace=True)\n",
    "if 'WEEK' in data_train.columns:\n",
    "    data_train.drop(['WEEK'],axis=1,inplace=True)\n",
    "if 'Nb_points_token' in data_train.columns:\n",
    "    data_train.drop(['Nb_points_token'],axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#on sauvegarde le fichier au format json\n",
    "data_train.to_json('/home/daril_kw/data/data_train.json',orient='records')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concaténation, padding et ajout tokens spéciaux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_inputs=data_train.CONTEXT_INPUT.values\n",
    "traj_inputs=data_train.DEB_TRAJ.values\n",
    "targets=data_train.TARGET.values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gestion de l'entrée : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_input_train(c_inputs,traj_inputs,targets):\n",
    "    \n",
    "    input_ids = []\n",
    "    full_inputs = []\n",
    "    attention_masks = []\n",
    "    for i in tqdm(range(len(c_inputs))):\n",
    "        #no truncation is needed because we managed it before\n",
    "\n",
    "        #we concatenate the context input and the trajectory input adding manually the CLS token and the SEP token\n",
    "        full_input = '[CLS] ' + c_inputs[i] + ' ' + traj_inputs[i] + ' [SEP]'\n",
    "        full_inputs.append(full_input)\n",
    "        #encoded_c_input=tokenizer.encode(c_inputs[i], add_special_tokens=False)\n",
    "        #encoded_traj_input=tokenizer.encode(traj_inputs[i], add_special_tokens=False)\n",
    "        #we add manually the CLS token and the SEP token when we concatenate the two inputs\n",
    "        #encoded_full_input=[101] + encoded_c_input + encoded_traj_input + [102]\n",
    "        #the[101] token is the CLS token and the [102] token is the SEP token\n",
    "\n",
    "        encoded_full_input=tokenizer.encode(full_input, add_special_tokens=False)\n",
    "        #we pad the input to the maximum length of 512\n",
    "        encoded_full_input=encoded_full_input + [0]*(512-len(encoded_full_input))\n",
    "        input_ids.append(encoded_full_input)\n",
    "        #we create the attention mask\n",
    "        att_mask = [float(i>0) for i in encoded_full_input]\n",
    "        attention_masks.append(att_mask)\n",
    "        #the attention mask is a list of 0 and 1, 0 for the padded tokens and 1 for the other tokens\n",
    "        #the float(i>0) is 0 if i=0 (ie if the token is a padded token) and 1 if i>0 (ie if the token is not a padded token)\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gestion des targets :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manage_targets_train(targets):\n",
    "    #on crée un dictionnaire qui associe à chaque target un entier compris entre 0 et len(targets_dict)-1\n",
    "    #on sauvegarde le dictionnaire au format json\n",
    "    #on\n",
    "    targets_dict={}\n",
    "    for i in range(len(targets)):\n",
    "        if targets[i] not in targets_dict:\n",
    "            targets_dict[targets[i]]=len(targets_dict)\n",
    "\n",
    "    targets_input=[targets_dict[targets[i]] for i in range(len(targets))]\n",
    "\n",
    "    #le dictionnaire est sauvegardé au format json\n",
    "    with open('/home/daril_kw/data/targets_dict.json', 'w') as fp:\n",
    "        json.dump(targets_dict, fp)\n",
    "\n",
    "# le dictionnaire s'utilisera comme suit : \n",
    "\n",
    "# soit un entier 'target' non compris entre 0 et len(targets_dict)-1\n",
    "# on récupère la valeur correspondante dans le dictionnaire avec targets_dict[target]\n",
    "# inversement, si on a un entier 'target_encoded' compris entre 0 et len(targets_dict)-1\n",
    "# on récupère la clé correspondante dans le dictionnaire avec list(targets_dict.keys())[target_encoded]\n",
    "\n",
    "#la liste targets_input contient les targets du dataset encodées avec le dictionnaire targets_dict ie dans leur nouvel espace\n",
    "#save in pickle the targets\n",
    "\n",
    "    with open('/home/daril_kw/data/targets_input.pkl', 'wb') as f:\n",
    "        pickle.dump(targets_input, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"data_train['DEB_TRAJ']=data_train['Tokenization_2'].apply(lambda x: x[:-NOMBRE])\n",
    "data_train['DEB_TRAJ']=data_train['DEB_TRAJ'].apply(lambda x: x[-(512-len_context_info-2):] if len(x)>512-len_context_info-2 else x)\n",
    "#then we keep the column in form of a string\n",
    "data_train['DEB_TRAJ']=data_train['DEB_TRAJ'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "\n",
    "data_train['TARGET']=data_train['Tokenization_2'].apply(lambda x: x[-NOMBRE:-1]) pour test\"\"\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gestion du dataframe de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_test(dataframe, first=True, last_predicted=None):\n",
    "    if first :\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
