{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import matthews_corrcoef\n",
    "from transformers import BertForSequenceClassification\n",
    "import torch\n",
    "import numpy as np\n",
    "import pickle\n",
    "from torch.utils.data import TensorDataset, DataLoader, SequentialSampler\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# This file test the first version of the model: classification with context\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "DIR_INPUTS_IDS = '/home/daril/trajcbert/savings_for_parrallel_1_2/input_ids_f_833383.pkl'\n",
    "DIR_ATTENTION_MASKS = '/home/daril/trajcbert/savings_for_parrallel_1_2/attention_masks_833383_opti.pkl'\n",
    "DIR_TARGETS = '/home/daril/trajcbert/savings_for_parrallel_1_2/targets_833383_opti.pkl'\n",
    "PRETRAINED_MODEL_NAME = '/home/daril/trajcbert//home/daril/scratch/data/trajcbert/models/model_saved_parallel_version_1_2_10_epochs'\n",
    "DATALOADER_DIR = '/home/daril/trajcbert/savings/test_dataloader_833383.pt'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# device = torch.device(\"cpu\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# load the prediction_dataloader\n",
    "prediction_dataloader = torch.load(DATALOADER_DIR)\n",
    "\n",
    "\n",
    "# we load the model\n",
    "model = BertForSequenceClassification.from_pretrained(PRETRAINED_MODEL_NAME)\n",
    "model.to(device)\n",
    "print(\"we evaluate\")\n",
    "model.eval()\n",
    "\n",
    "# Tracking variables\n",
    "predictions, true_labels, list_inputs_test = [], [], []\n",
    "\n",
    "# losses\n",
    "losses = 0\n",
    "print(\"We predict\")\n",
    "# Predict\n",
    "for batch in prediction_dataloader:\n",
    "    # Add batch to GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "    # move to device\n",
    "    b_input_ids = b_input_ids.to(device)\n",
    "    b_input_mask = b_input_mask.to(device)\n",
    "    b_labels = b_labels.to(device)\n",
    "\n",
    "    # Telling the model not to compute or store gradients, saving memory and\n",
    "    # speeding up prediction\n",
    "    with torch.no_grad():\n",
    "        # Forward pass, calculate logit predictions\n",
    "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "        # the ouputs are a tuple with the loss and the logits\n",
    "        # the losses are the item 0 of the tuple\n",
    "        # and the logits are the item 1 of the tuple\n",
    "        # The loss is computed with the CrossEntropyLoss\n",
    "\n",
    "    logits = outputs[0]\n",
    "    losses += outputs[0].mean().item()\n",
    "\n",
    "    # Move logits and labels to CPU\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to(\"cpu\").numpy()\n",
    "\n",
    "    # Store predictions and true labels\n",
    "    # we have to append  the max of the logits\n",
    "    # because the logits are the output of the softmax\n",
    "    # and the max of the logits is the class with the highest probability\n",
    "    predictions.append(logits)\n",
    "    true_labels.append(label_ids)\n",
    "\n",
    "    # Store the inputs\n",
    "\n",
    "    list_inputs_test.append(b_input_ids.tolist())\n",
    "\n",
    "print(\"DONE.\")\n",
    "\n",
    "\n",
    "matthews_set = []\n",
    "\n",
    "# Evaluate each test batch using Matthew's correlation coefficient\n",
    "print(\"Calculating Matthews Corr. Coef. for each batch...\")\n",
    "\n",
    "pred_label = []\n",
    "# compute the loss\n",
    "\n",
    "# For each input batch...\n",
    "for i in range(len(true_labels)):\n",
    "    # The predictions for this batch are a 2-column ndarray (one column for \"0\"\n",
    "    # and one column for \"1\"). Pick the label with the highest value and turn this\n",
    "    # in to a list of 0s and 1s.\n",
    "    pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n",
    "    pred_label.append(pred_labels_i)\n",
    "    # Calculate and store the coef for this batch.\n",
    "    matthews = matthews_corrcoef(true_labels[i], pred_labels_i)\n",
    "    matthews_set.append(matthews)\n",
    "\n",
    "\n",
    "# Combine the predictions for each batch into a single list of 0s and 1s.\n",
    "flat_predictions = [item for sublist in predictions for item in sublist]\n",
    "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
    "\n",
    "# Combine the correct labels for each batch into a single list.\n",
    "flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
    "\n",
    "\n",
    "# Combine the inputs for each batch into a single list.\n",
    "flat_list_inputs_test = [item for sublist in list_inputs_test for item in sublist]\n",
    "\n",
    "# Calculate the MCC\n",
    "mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\n",
    "\n",
    "print(\"MCC: %.3f\" % mcc)\n",
    "\n",
    "\n",
    "# compute the accuracy\n",
    "accuracy = (flat_true_labels == flat_predictions).mean()\n",
    "print(\"accuracy: %.3f\" % accuracy)\n",
    "\n",
    "# print the loss\n",
    "print(\"loss: %.3f\" % (losses / len(true_labels)))\n",
    "\n",
    "\n",
    "# save flat_list_inputs_test\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
