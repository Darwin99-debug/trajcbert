{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from sklearn.metrics import f1_score\n",
    "import random\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "from sklearn.metrics import f1_score\n",
    "import h3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization à partit du dataset nettoyé \n",
    "### (ie sans les trajets trop courts)\n",
    "\n",
    "On commence par charger le dataset0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/daril_kw/data/02.06.23/train_clean.json', 'r') as openfile:\n",
    "\n",
    "    # Reading from json file\n",
    "    json_loaded = json.load(openfile)\n",
    "\n",
    "print(\"We put the data in a dataset.\")\n",
    " \n",
    "\n",
    "#we put them in a dataframe\n",
    "data_format = pd.DataFrame(data=json_loaded)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On fait la tokenization correcte (ordre latitude, longitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_format['Tokenization_2'] = data_format['POLYLINE'].apply(lambda x: [h3.geo_to_h3(x[i][0], x[i][1], 10) for i in range(len(x))])\n",
    "\n",
    "#on enregistre dans un json le dataframe\n",
    "data_format.to_json('/home/daril_kw/data/data_2_without_time_info.json',orient='records')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gestion du tokenizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le but est d'ajouter au tokenizer les jetons géographiques et de contexte."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction des données temporelles du timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#on commence par lire le json\n",
    "\n",
    "with open('/home/daril_kw/data/data_2_without_time_info.json', 'r') as openfile:\n",
    "    # Reading from json file\n",
    "    data_format = json.load(openfile)\n",
    "\n",
    "data_format = pd.DataFrame(data=data_format)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On ajoute les colonnes de contexte temporel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#on récupère la date à partir du timestamp\n",
    "data_format['DATE'] = data_format['TIMESTAMP'].apply(lambda x: datetime.datetime.fromtimestamp(x).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "#a partir de cela on récupère le jour de la semaine sous forme d'un nombre entre 1 et 7 pour lundi à dimanche\n",
    "data_format['DAY'] = data_format['DATE'].apply(lambda x: str(datetime.datetime.strptime(x.split(' ')[0],'%Y-%m-%d').isocalendar()[2]))\n",
    "\n",
    "#ensuite, on récupère l'heure sous forme d'un nombre entre 0 et 23\n",
    "data_format['HOUR'] = data_format['DATE'].apply(lambda x: x.split(' ')[1].split(':')[0])\n",
    "\n",
    "#enfin on recupère le numéro de la semaine dans l'année\n",
    "data_format['WEEK'] = data_format['DATE'].apply(lambda x: str(datetime.datetime.strptime(x.split(' ')[0],'%Y-%m-%d').isocalendar()[1]))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On enlève les colonnes inutiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean.drop(['MISSING_DATA','DATE','ORIGIN_CALL','TRIP_ID', 'DAY_TYPE', 'ORIGIN_CALL', 'ORIGIN_STAND', 'Nb_points', 'TIMESTAMP' ],axis=1,inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On met dans le bon format les autres données de contexte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_format['CALL_TYPE'] = data_format['CALL_TYPE'].apply(lambda x: str(1) if x=='A' else str(2) if x=='B' else str(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if type(data_format['TAXI_ID'][0])!=str:\n",
    "    data_format['TAXI_ID']=data_format['TAXI_ID'].apply(lambda x: str(x))\n",
    "#idem pour le call_type\n",
    "if type(data_format['CALL_TYPE'][0])!=str:\n",
    "    data_format['CALL_TYPE']=data_format['CALL_TYPE'].apply(lambda x: str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On sauvegarde le dataframe\n",
    "data_format.to_json('/home/daril_kw/data/data_with_time_info_ok.json',orient='records')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On ajoute au vocabulaires tous les tokens dont on a besoin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#on charge le tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On commence par s'occuper des tokens géographiques issus de la tokenization h3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liste_token_geo = []\n",
    "\n",
    "for i in range(len(data_format)):\n",
    "    for j in range(len(data_format['Tokenization_2'][i])):\n",
    "        liste_token_geo.append(data_format['Tokenization_2'][i][j])\n",
    "\n",
    "#on enlève les doublons\n",
    "liste_token_geo = list(set(liste_token_geo))\n",
    "\n",
    "#on en profite pour sauvegarder la liste des tokens géographiques dans un fichier txt\n",
    "with open('/home/daril_kw/data/liste_token_geo.txt', 'w') as f:\n",
    "    for item in liste_token_geo:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "\n",
    "#on garde le nombre de tokens géographiques pour la suite\n",
    "nb_token_geo = len(liste_token_geo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On ajoute les tokens géographiques au tokenizer\n",
    "tokenizer.add_tokens(liste_token_geo)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On s'occupe maintenant des tokens de contexte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contextual_info_token = []\n",
    "\n",
    "for i in range(len(data_format)):\n",
    "    contextual_info_token.append(data_format['CALL_TYPE'][i])\n",
    "    contextual_info_token.append(data_format['TAXI_ID'][i])\n",
    "    contextual_info_token.append(data_format['DAY'][i])\n",
    "    contextual_info_token.append(data_format['HOUR'][i])\n",
    "    contextual_info_token.append(data_format['WEEK'][i])\n",
    "\n",
    "#on enlève les doublons\n",
    "contextual_info_token = list(set(contextual_info_token))\n",
    "\n",
    "tokenizer.add_tokens(contextual_info_token)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On sauvegarde le tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained('/home/daril_kw/data/tokenizer_final')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gestion du modèle"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On charge le modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On a besoin du nombre de labels, celui-ci correspond au nombre de tokens géographiques + 1 (pour le token [SEQ] indiquant la fin de la séquence)\n",
    "\n",
    "nb_labels = nb_token_geo + 1\n",
    "\n",
    "model=BertForSequenceClassification.from_pretrained(\"bert-base-cased\",num_labels=nb_labels)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On adapte la taille de l'embedding au tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#on adapte la taille de l'embedding pour qu'elle corresponde au nombre de tokens géographiques + 1\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On sauvegarde le modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On sauvegarde le modèle\n",
    "model.save_pretrained('/home/daril_kw/data/model_final')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gestion de l'input (formattage)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour une tâche de classification, nous avons besoin que l'entrée soit sous une forme particulière. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/daril_kw/data//home/daril_kw/data/data_with_time_info_ok.json', 'r') as openfile:\n",
    "    json_loaded = json.load(openfile)\n",
    "    \n",
    "data_format = pd.DataFrame(data=json_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#on ajoute un espace devant les éléments de la colonne HOUR, WEEK, CALL_TYPE, TAXI_ID en prévision de la concaténation pour séparer d'un espace les éléments de la colonne Tokenization et les autres colonnes\n",
    "\n",
    "data_format['HOUR']=data_format['HOUR'].apply(lambda x: ' '+x)\n",
    "data_format['WEEK']=data_format['WEEK'].apply(lambda x: ' '+x)\n",
    "data_format['CALL_TYPE']=data_format['CALL_TYPE'].apply(lambda x: ' '+x)\n",
    "data_format['TAXI_ID']=data_format['TAXI_ID'].apply(lambda x: ' '+x)\n",
    "data_format['DAY']=data_format['DAY'].apply(lambda x: ' '+x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# la colonne CONTEXT_INPUT sera la concaténation du jour de la semaine, de l'heure et de la semaien de l'année pui de la colonne CALL_TYPE, de la colonne TAXI_ID, d'un espace et du dernier token de la colonne Tokenization\n",
    "data_format['CONTEXT_INPUT'] =data_format['Tokenization_2'].apply(lambda x: x[-1]) + data_format['DAY'] + data_format['HOUR'] + data_format['WEEK'] + data_format['CALL_TYPE'] + data_format['TAXI_ID']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#on récupère le nombre d'informations dans la colonne CONTEXT_INPUT\n",
    "#Comme cette colonne contiient les informations en string séparé par un espace, on récupère la liste correspondante puis on compte le nombre d'éléments de cette liste\n",
    "len_context_info = len(data_format['CONTEXT_INPUT'][0].split(' '))\n",
    "\n",
    "#la colonne DEB_TRAJ sera la colonne Tokenization jusqu'a l'avant-dernier token exclu\n",
    "\n",
    "data_format['DEB_TRAJ']=data_format['Tokenization_2'].apply(lambda x: x[:-2])\n",
    "\n",
    "\n",
    "# on gère la longueur de la colonne CONTEXT_INPUT pour qu'après la concaténation, elle ne dépasse pas 512 tokens\n",
    "#le -2 correspond aux deux tokens spéciaux [CLS] et [SEP]\n",
    "# for exemple here, if the trajectory input is too long, we keep the 512-6-2=504 last tokens\n",
    "data_format['DEB_TRAJ']=data_format['DEB_TRAJ'].apply(lambda x: x[-(512-len_context_info-2):] if len(x)>512-len_context_info-2 else x)\n",
    "\n",
    "#then we keep the column in form of a string\n",
    "data_format['DEB_TRAJ']=data_format['DEB_TRAJ'].apply(lambda x: ' '.join(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_format['TARGET']=data_format['Tokenization_2'].apply(lambda x: x[-2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#on enlève les colonnes inutiles\n",
    "if 'Tokenization' in data_format.columns:\n",
    "    data_format.drop(['Tokenization'],axis=1,inplace=True)\n",
    "if 'CALL_TYPE' in data_format.columns:\n",
    "    data_format.drop(['CALL_TYPE'],axis=1,inplace=True)\n",
    "if 'TAXI_ID' in data_format.columns:\n",
    "    data_format.drop(['TAXI_ID'],axis=1,inplace=True)\n",
    "if 'DAY' in data_format.columns:\n",
    "    data_format.drop(['DAY'],axis=1,inplace=True)\n",
    "if 'HOUR' in data_format.columns:\n",
    "    data_format.drop(['HOUR'],axis=1,inplace=True)\n",
    "if 'WEEK' in data_format.columns:\n",
    "    data_format.drop(['WEEK'],axis=1,inplace=True)\n",
    "if 'Nb_points_token' in data_format.columns:\n",
    "    data_format.drop(['Nb_points_token'],axis=1,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#on sauvegarde le fichier au format json\n",
    "data_format.to_json('/home/daril_kw/data/data_formatted_final.json',orient='records')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concaténation, padding et ajout tokens spéciaux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_inputs=data_format.CONTEXT_INPUT.values\n",
    "traj_inputs=data_format.DEB_TRAJ.values\n",
    "targets=data_format.TARGET.values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gestion de l'entrée : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = []\n",
    "full_inputs = []\n",
    "attention_masks = []\n",
    "for i in tqdm(range(len(c_inputs))):\n",
    "    #no truncation is needed because we managed it before\n",
    "\n",
    "    #we concatenate the context input and the trajectory input adding manually the CLS token and the SEP token\n",
    "    full_input = '[CLS] ' + c_inputs[i] + ' ' + traj_inputs[i] + ' [SEP]'\n",
    "    full_inputs.append(full_input)\n",
    "    #encoded_c_input=tokenizer.encode(c_inputs[i], add_special_tokens=False)\n",
    "    #encoded_traj_input=tokenizer.encode(traj_inputs[i], add_special_tokens=False)\n",
    "    #we add manually the CLS token and the SEP token when we concatenate the two inputs\n",
    "    #encoded_full_input=[101] + encoded_c_input + encoded_traj_input + [102]\n",
    "    #the[101] token is the CLS token and the [102] token is the SEP token\n",
    "\n",
    "    encoded_full_input=tokenizer.encode(full_input, add_special_tokens=False)\n",
    "    #we pad the input to the maximum length of 512\n",
    "    encoded_full_input=encoded_full_input + [0]*(512-len(encoded_full_input))\n",
    "    input_ids.append(encoded_full_input)\n",
    "    #we create the attention mask\n",
    "    att_mask = [float(i>0) for i in encoded_full_input]\n",
    "    attention_masks.append(att_mask)\n",
    "    #the attention mask is a list of 0 and 1, 0 for the padded tokens and 1 for the other tokens\n",
    "    #the float(i>0) is 0 if i=0 (ie if the token is a padded token) and 1 if i>0 (ie if the token is not a padded token)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gestion des targets :"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le problème est que pour les labels, nous devons entrer des entiers entre 0 et len(targets)-1.\n",
    "\n",
    "Pour cela, on  crée un dictionnaire qui associe à chaque label un entier entre 0 et len(targets)-1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_dict={}\n",
    "for i in range(len(targets)):\n",
    "    if targets[i] not in targets_dict:\n",
    "        targets_dict[targets[i]]=len(targets_dict)\n",
    "\n",
    "targets_input=[targets_dict[targets[i]] for i in range(len(targets))]\n",
    "\n",
    "#le dictionnaire est sauvegardé au format json\n",
    "with open('/home/daril_kw/data/targets_dict.json', 'w') as fp:\n",
    "    json.dump(targets_dict, fp)\n",
    "\n",
    "# le dictionnaire s'utilisera comme suit : \n",
    "\n",
    "# soit un entier 'target' non compris entre 0 et len(targets_dict)-1\n",
    "# on récupère la valeur correspondante dans le dictionnaire avec targets_dict[target]\n",
    "# inversement, si on a un entier 'target_encoded' compris entre 0 et len(targets_dict)-1\n",
    "# on récupère la clé correspondante dans le dictionnaire avec list(targets_dict.keys())[target_encoded]\n",
    "\n",
    "#la liste targets_input contient les targets du dataset encodées avec le dictionnaire targets_dict ie dans leur nouvel espace\n",
    "#save in pickle the targets\n",
    "\n",
    "with open('/home/daril_kw/data/targets_input.pkl', 'wb') as f:\n",
    "    pickle.dump(targets_input, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#on separe les données en train, validation et test\n",
    "\n",
    "train_data, test_input, train_targets, test_targets = train_test_split(input_ids, targets_input,random_state=2023, test_size=0.2) \n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(train_data, train_targets,random_state=2023, test_size=0.1)\n",
    "\n",
    "train_masks, test_masks, _, _ = train_test_split(attention_masks, targets_input,random_state=2023, test_size=0.2)\n",
    "#train_masks, test_masks, _, _ = train_test_split(attention_masks, targets_input,random_state=2023, test_size=0.1)\n",
    "train_masks, validation_masks, _, _ = train_test_split(train_masks, train_targets,random_state=2023, test_size=0.1)\n",
    "\n",
    "\n",
    "#on convertit les données en tenseurs\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "test_inputs = torch.tensor(test_input)\n",
    "\n",
    "train_labels = torch.tensor(train_labels)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "test_labels = torch.tensor(test_targets)\n",
    "\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_masks = torch.tensor(validation_masks)\n",
    "test_masks = torch.tensor(test_masks)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataloading :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "# Create the DataLoader for our training set, one for validation set and one for test set\n",
    "\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data,sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data,sampler=validation_sampler, batch_size=batch_size)\n",
    "\n",
    "prediction_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "prediction_sampler = SequentialSampler(prediction_data)\n",
    "prediction_dataloader = DataLoader(prediction_data,sampler=prediction_sampler, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\"/home/daril_kw/data/model_final\",num_labels=nb_labels)\n",
    "model.to(device)\n",
    "model = DistributedDataParallel(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(),lr = 2e-5,eps = 1e-8)\n",
    "\n",
    "# Number of training epochs. The BERT authors recommend between 2 and 4.\n",
    "epochs = 4\n",
    "\n",
    "# Total number of training steps is number of batches * number of epochs.\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,num_warmup_steps = 0,num_training_steps = total_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#on définit les fonctions utiles pour l'entrainement\n",
    "\n",
    "\n",
    "#la focntion\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "\n",
    "def format_time(elapsed):\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n",
    "def flat_f1(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return f1_score(labels_flat,pred_flat,average='macro')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_val = 2023\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we store the loss and accuracy of each epoch\n",
    "loss_values = []\n",
    "accuracy_values = []\n",
    "f1_values = []\n",
    "\n",
    "# For each epoch...\n",
    "for epoch_i in range(0, epochs):\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "    t0 = time.time()\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # Progress update every 40 batches.\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader),elapsed))\n",
    "        \n",
    "\n",
    "        \"\"\"\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\"\"\"\n",
    "\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "  \n",
    "        # Unpack the inputs from our dataloader\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "\n",
    "        #we set the gradients to zero\n",
    "        model.zero_grad()\n",
    "        #we make the forward pass\n",
    "        outputs = model(b_input_ids,token_type_ids=None,attention_mask=b_input_mask,labels=b_labels)\n",
    "        #we get the loss\n",
    "        loss = outputs[0]\n",
    "        #we accumulate the loss\n",
    "        total_loss += loss.item()\n",
    "        #we make the backward pass\n",
    "        loss.backward()\n",
    "        #we clip the gradient to avoid exploding gradient\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        #we update the parameters\n",
    "        optimizer.step()\n",
    "        #we update the learning rate\n",
    "        scheduler.step()\n",
    "    # Calculate the average loss over all of the batches.  \n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    # Store the loss value for plotting the learning curve.\n",
    "    loss_values.append(avg_train_loss)\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epoch took: {:}\".format(format_time(time.time() - t0)))\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "    t0 = time.time()\n",
    "    # Put the model in evaluation mode--the dropout layers behave differently during evaluation.\n",
    "    model.eval()\n",
    "    # Tracking variables\n",
    "    eval_loss, eval_accuracy,eval_f1 = 0, 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        #we unpack the batch\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        #we don't compute the gradient\n",
    "        with torch.no_grad():\n",
    "            #we make the forward pass\n",
    "            outputs = model(b_input_ids,token_type_ids=None,attention_mask=b_input_mask)\n",
    "            #we get the logits\n",
    "            logits = outputs[0]\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        #we compute the accuracy\n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "        #we compute the f1 score\n",
    "        tmp_eval_f1 = flat_f1(logits, label_ids)\n",
    "        #we accumulate the accuracy\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "        #we accumulate the f1 score\n",
    "        eval_f1 += tmp_eval_f1\n",
    "        #we accumulate the number of examples\n",
    "        nb_eval_examples += b_input_ids.size(0)\n",
    "        #we accumulate the number of steps\n",
    "        nb_eval_steps += 1\n",
    "    #we compute the accuracy\n",
    "    eval_accuracy = eval_accuracy / nb_eval_examples\n",
    "    #we compute the f1 score\n",
    "    eval_f1 = eval_f1 / nb_eval_examples\n",
    "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy))\n",
    "    print(\"  F1 score: {0:.2f}\".format(eval_f1))\n",
    "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
    "    #we store the accuracy\n",
    "    accuracy_values.append(eval_accuracy)\n",
    "    #we store the f1 score\n",
    "    f1_values.append(eval_f1)\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "\n",
    "#in the trainning loop, the loss is computed for each batch, so we have to compute the average loss for each epoch\n",
    "#the loss function is the cross entropy loss that is to say the negative log likelihood loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we save the model\n",
    "output_dir = './model_save/'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "print(\"Saving model to %s\" % output_dir)\n",
    "model_to_save = model.module if hasattr(model, 'module') else model\n",
    "model_to_save.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "#we save the loss and accuracy values\n",
    "np.save(output_dir+'loss_values.npy',loss_values)\n",
    "np.save(output_dir+'accuracy_values.npy',accuracy_values)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    " \n",
    "# Tracking variables \n",
    "predictions , true_labels, list_inputs_test = [], [], []\n",
    "print(\"We predict\")\n",
    "# Predict \n",
    "for batch in prediction_dataloader:\n",
    "  # Add batch to GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "   \n",
    "  # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "  \n",
    "  # Telling the model not to compute or store gradients, saving memory and \n",
    "  # speeding up prediction\n",
    "    with torch.no_grad():\n",
    "      # Forward pass, calculate logit predictions\n",
    "        outputs = model(b_input_ids, token_type_ids=None, \n",
    "                      attention_mask=b_input_mask)\n",
    "\n",
    "    logits = outputs[0]\n",
    "\n",
    "  # Move logits and labels to CPU\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "  \n",
    "  # Store predictions and true labels\n",
    "    predictions.append(logits)\n",
    "    true_labels.append(label_ids)\n",
    "\n",
    "  #Store the inputs\n",
    "\n",
    "    list_inputs_test.append(b_input_ids.tolist())\n",
    "\n",
    "print('    DONE.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matthews_set = []\n",
    "\n",
    "# Evaluate each test batch using Matthew's correlation coefficient\n",
    "print('Calculating Matthews Corr. Coef. for each batch...')\n",
    "\n",
    "pred_label= []\n",
    "# For each input batch...\n",
    "for i in range(len(true_labels)):\n",
    "\n",
    "  # The predictions for this batch are a 2-column ndarray (one column for \"0\"\n",
    "  # and one column for \"1\"). Pick the label with the highest value and turn this\n",
    "  # in to a list of 0s and 1s.\n",
    "  pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n",
    "  pred_label.append(pred_labels_i)\n",
    "  # Calculate and store the coef for this batch.\n",
    "  matthews = matthews_corrcoef(true_labels[i], pred_labels_i)\n",
    "  matthews_set.append(matthews)\n",
    "\n",
    "\n",
    "# Combine the predictions for each batch into a single list of 0s and 1s.\n",
    "flat_predictions = [item for sublist in predictions for item in sublist]\n",
    "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
    "\n",
    "# Combine the correct labels for each batch into a single list.\n",
    "flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
    "\n",
    "\n",
    "# Combine the inputs for each batch into a single list.\n",
    "flat_list_inputs_test = [item for sublist in list_inputs_test for item in sublist]\n",
    "\n",
    "# Calculate the MCC\n",
    "mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\n",
    "\n",
    "print('MCC: %.3f' % mcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #display the confusion matrix\n",
    "#cm = confusion_matrix(flat_true_labels, flat_predictions)\n",
    "#print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#we begin with the correct trajectory\n",
    "#we get the input ids\n",
    "correct_input_ids = [ [] for i in range(len(flat_list_inputs_test))]\n",
    "#in the input, the part that interests us is the part after the cls token and after the six context tokens (so the last 512-len_context_info-1 tokens)\n",
    "\n",
    "#we remove the padding tokens in the list flat_list_inputs_test\n",
    "for i in range(len(flat_list_inputs_test)):\n",
    "    for j in range(len(flat_list_inputs_test[i])):\n",
    "        if flat_list_inputs_test[i][j] == 0:\n",
    "            flat_list_inputs_test[i] = flat_list_inputs_test[i][:j]\n",
    "            break\n",
    "\n",
    "\n",
    "for i in range(len(flat_list_inputs_test)):\n",
    "    correct_input_ids[i].append(flat_list_inputs_test[i][7:len(flat_list_inputs_test[i])-1])\n",
    "\n",
    "    #the 7 is because we have the cls token and the six context tokens\n",
    "    #the len(list_inputs_test[i][0])-1 is because we have the sep token at the end\n",
    "    #we flatten\n",
    "\n",
    "    correct_input_ids[i] = [item for sublist in correct_input_ids[i] for item in sublist]\n",
    "\n",
    "    #we get the label that is to say the penultimate point of the trajectory\n",
    "    correct_input_ids[i].append(list(targets_dict.keys())[flat_true_labels[i]])\n",
    "\n",
    "    #we get the destination ie the last point of the trajectory that is to say the first token after the cls token in the input\n",
    "    correct_input_ids[i].append(flat_list_inputs_test[i][1])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#we get the same thing but with the predicted labels instead of the true labels\n",
    "incorrect_input_ids = [ [] for i in range(len(flat_list_inputs_test))]\n",
    "for i in range(len(flat_list_inputs_test)):\n",
    "    \n",
    "    incorrect_input_ids[i].append(flat_list_inputs_test[i][7:len(flat_list_inputs_test[i])-1])\n",
    "\n",
    "    #we don't want the list format so we flatten the list\n",
    "    incorrect_input_ids[i] = [item for sublist in incorrect_input_ids[i] for item in sublist]\n",
    "\n",
    "    #we have to use the dict to get the corresponding label\n",
    "    incorrect_input_ids[i].append(list(targets_dict.keys())[flat_predictions[i]])\n",
    "    incorrect_input_ids[i].append(flat_list_inputs_test[i][1])\n",
    "    \n",
    "\n",
    "#we have numbers but we want to get the corresponding words\n",
    "#we get the words\n",
    "\n",
    "#we do that for the incorrect trajectories\n",
    "incorrect_words = []\n",
    "for i in range(len(incorrect_input_ids)):\n",
    "    incorrect_words.append([])\n",
    "    for j in range(len(incorrect_input_ids[i])):\n",
    "        incorrect_words[i].append(tokenizer.convert_ids_to_tokens(incorrect_input_ids[i][j]))\n",
    "        #incorrect_words[i][j] is the jth word of the ith trajectory\n",
    "\n",
    "\n",
    "\n",
    "#correct trajectories\n",
    "correct_words = []\n",
    "for i in range(len(correct_input_ids)):\n",
    "    correct_words.append([])\n",
    "    for j in range(len(correct_input_ids[i])):\n",
    "        correct_words[i].append(tokenizer.convert_ids_to_tokens(correct_input_ids[i][j]))\n",
    "        #correct_words[i][j] is the jth word of the ith trajectory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "import h3\n",
    "\n",
    "reso = 10\n",
    "\n",
    "map_center = [41.157943, -8.629105] # Porto coordinates\n",
    "m = folium.Map(location=map_center, zoom_start=12)\n",
    "\n",
    "h3_tokens= []\n",
    "for i in range(len(correct_words)):\n",
    "    for j in range(len(correct_words[i])):\n",
    "        h3_tokens.append(correct_words[i][j])\n",
    "\n",
    "h3_tokens= list(set(h3_tokens))\n",
    "\n",
    "hexagon_vertices = []\n",
    "for h3_token in h3_tokens:\n",
    "    hexagon_vertices.append(h3.h3_to_geo_boundary(h3_token))\n",
    "\n",
    "for i in range(len(hexagon_vertices)): \n",
    "    folium.Polygon(locations=hexagon_vertices[i],color='green',fill=True,fill_color='green',fill_opacity=0.4).add_to(m)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display the trajectories\n",
    "#for that we have du detokenize with h3_to_geo from the h3 library\n",
    "\n",
    "#the trajectories are in the correct_words list\n",
    "for traj in correct_words:\n",
    "    #we get the coordinates of the trajectory\n",
    "    coords = []\n",
    "    for token in traj:\n",
    "        coords.append(h3.h3_to_geo(token))\n",
    "    folium.PolyLine(coords, color=\"blue\", weight=2.5, opacity=1).add_to(m)\n",
    "\n",
    "    for i in range(len(traj)):\n",
    "        folium.CircleMarker(traj[i], radius=1, color='blue', fill=True, fill_color='blue', fill_opacity=0.6).add_to(m)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display the map\n",
    "m"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
