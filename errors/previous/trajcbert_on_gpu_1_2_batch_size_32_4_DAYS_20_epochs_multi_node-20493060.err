master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
INFO:torch.distributed.launcher.api:Starting elastic_operator with launch configs:
  entrypoint       : parallelisation_gpu_train_torch_run_multinode.py
  min_nodes        : 3
  max_nodes        : 3
  nproc_per_node   : 1
  run_id           : 28924
  rdzv_backend     : c10d
  rdzv_endpoint    : 10.80.6.102:29500
  rdzv_configs     : {'timeout': 900}
  max_restarts     : 0
  monitor_interval : 5
  log_dir          : None
  metrics_cfg      : {}

INFO:torch.distributed.elastic.agent.server.local_elastic_agent:log directory set to: /tmp/torchelastic_mqo9vqua/28924_9bggjw2h
INFO:torch.distributed.elastic.agent.server.api:[default] starting workers for entrypoint: python
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
INFO:torch.distributed.launcher.api:Starting elastic_operator with launch configs:
  entrypoint       : parallelisation_gpu_train_torch_run_multinode.py
  min_nodes        : 3
  max_nodes        : 3
  nproc_per_node   : 1
  run_id           : 28924
  rdzv_backend     : c10d
  rdzv_endpoint    : 10.80.6.102:29500
  rdzv_configs     : {'timeout': 900}
  max_restarts     : 0
  monitor_interval : 5
  log_dir          : None
  metrics_cfg      : {}

INFO:torch.distributed.launcher.api:Starting elastic_operator with launch configs:
  entrypoint       : parallelisation_gpu_train_torch_run_multinode.py
  min_nodes        : 3
  max_nodes        : 3
  nproc_per_node   : 1
  run_id           : 28924
  rdzv_backend     : c10d
  rdzv_endpoint    : 10.80.6.102:29500
  rdzv_configs     : {'timeout': 900}
  max_restarts     : 0
  monitor_interval : 5
  log_dir          : None
  metrics_cfg      : {}

INFO:torch.distributed.elastic.agent.server.local_elastic_agent:log directory set to: /tmp/torchelastic_65zfb6v3/28924_ew9d16_9
INFO:torch.distributed.elastic.agent.server.local_elastic_agent:log directory set to: /tmp/torchelastic_vt41usku/28924_llb60ijz
INFO:torch.distributed.elastic.agent.server.api:[default] starting workers for entrypoint: python3.10
INFO:torch.distributed.elastic.agent.server.api:[default] starting workers for entrypoint: python3.10
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=0
  master_addr=ng10602.narval.calcul.quebec
  master_port=56953
  group_rank=0
  group_world_size=3
  local_ranks=[0]
  role_ranks=[0]
  global_ranks=[0]
  role_world_sizes=[3]
  global_world_sizes=[3]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.agent.server.local_elastic_agent:Environment variable 'TORCHELASTIC_ENABLE_FILE_TIMER' not found. Do not start FileTimerServer.
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_mqo9vqua/28924_9bggjw2h/attempt_0/0/error.json
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=0
  master_addr=ng10602.narval.calcul.quebec
  master_port=56953
  group_rank=1
  group_world_size=3
  local_ranks=[0]
  role_ranks=[1]
  global_ranks=[1]
  role_world_sizes=[3]
  global_world_sizes=[3]

INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=0
  master_addr=ng10602.narval.calcul.quebec
  master_port=56953
  group_rank=2
  group_world_size=3
  local_ranks=[0]
  role_ranks=[2]
  global_ranks=[2]
  role_world_sizes=[3]
  global_world_sizes=[3]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.agent.server.local_elastic_agent:Environment variable 'TORCHELASTIC_ENABLE_FILE_TIMER' not found. Do not start FileTimerServer.
INFO:torch.distributed.elastic.agent.server.local_elastic_agent:Environment variable 'TORCHELASTIC_ENABLE_FILE_TIMER' not found. Do not start FileTimerServer.
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_vt41usku/28924_llb60ijz/attempt_0/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_65zfb6v3/28924_ew9d16_9/attempt_0/0/error.json
Traceback (most recent call last):
  File "/home/daril/trajcbert/parallelisation_gpu_train_torch_run_multinode.py", line 375, in <module>
    main(
  File "/home/daril/trajcbert/parallelisation_gpu_train_torch_run_multinode.py", line 342, in main
    trainer = Trainer(
  File "/home/daril/trajcbert/parallelisation_gpu_train_torch_run_multinode.py", line 112, in __init__
    self._load_snapshot(snapshot_path)
  File "/home/daril/trajcbert/parallelisation_gpu_train_torch_run_multinode.py", line 121, in _load_snapshot
    snapshot = torch.load(snapshot_path)
  File "/localscratch/daril.20493060.0/MYENV/lib/python3.10/site-packages/torch/serialization.py", line 797, in load
    with _open_zipfile_reader(opened_file) as opened_zipfile:
  File "/localscratch/daril.20493060.0/MYENV/lib/python3.10/site-packages/torch/serialization.py", line 283, in __init__
    super().__init__(torch._C.PyTorchFileReader(name_or_buffer))
RuntimeError: PytorchStreamReader failed reading zip archive: failed finding central directory
Traceback (most recent call last):
  File "/home/daril/trajcbert/parallelisation_gpu_train_torch_run_multinode.py", line 375, in <module>
Traceback (most recent call last):
  File "/home/daril/trajcbert/parallelisation_gpu_train_torch_run_multinode.py", line 375, in <module>
    main(
    main(
  File "/home/daril/trajcbert/parallelisation_gpu_train_torch_run_multinode.py", line 342, in main
  File "/home/daril/trajcbert/parallelisation_gpu_train_torch_run_multinode.py", line 342, in main
    trainer = Trainer(
  File "/home/daril/trajcbert/parallelisation_gpu_train_torch_run_multinode.py", line 112, in __init__
    trainer = Trainer(
  File "/home/daril/trajcbert/parallelisation_gpu_train_torch_run_multinode.py", line 112, in __init__
    self._load_snapshot(snapshot_path)
  File "/home/daril/trajcbert/parallelisation_gpu_train_torch_run_multinode.py", line 121, in _load_snapshot
    self._load_snapshot(snapshot_path)
  File "/home/daril/trajcbert/parallelisation_gpu_train_torch_run_multinode.py", line 121, in _load_snapshot
    snapshot = torch.load(snapshot_path)
  File "/home/daril/.local/lib/python3.10/site-packages/torch/serialization.py", line 797, in load
    snapshot = torch.load(snapshot_path)
  File "/home/daril/.local/lib/python3.10/site-packages/torch/serialization.py", line 797, in load
    with _open_zipfile_reader(opened_file) as opened_zipfile:
    with _open_zipfile_reader(opened_file) as opened_zipfile:
  File "/home/daril/.local/lib/python3.10/site-packages/torch/serialization.py", line 283, in __init__
  File "/home/daril/.local/lib/python3.10/site-packages/torch/serialization.py", line 283, in __init__
    super().__init__(torch._C.PyTorchFileReader(name_or_buffer))
RuntimeError: PytorchStreamReader failed reading zip archive: failed finding central directory
    super().__init__(torch._C.PyTorchFileReader(name_or_buffer))
RuntimeError: PytorchStreamReader failed reading zip archive: failed finding central directory
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 2496088) of binary: /localscratch/daril.20493060.0/MYENV/bin/python
INFO:torch.distributed.elastic.agent.server.api:Local worker group finished (FAILED). Waiting 300 seconds for other agents to finish
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 3927265) of binary: /cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.10.2/bin/python3.10
INFO:torch.distributed.elastic.agent.server.api:Local worker group finished (FAILED). Waiting 300 seconds for other agents to finish
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 3848166) of binary: /cvmfs/soft.computecanada.ca/easybuild/software/2020/avx2/Core/python/3.10.2/bin/python3.10
INFO:torch.distributed.elastic.agent.server.api:Local worker group finished (FAILED). Waiting 300 seconds for other agents to finish
INFO:torch.distributed.elastic.agent.server.api:Done waiting for other agents. Elapsed: 0.001039266586303711 seconds
INFO:torch.distributed.elastic.agent.server.api:Done waiting for other agents. Elapsed: 0.0010104179382324219 seconds
INFO:torch.distributed.elastic.agent.server.api:Done waiting for other agents. Elapsed: 5.0108349323272705 seconds
INFO:torch.distributed.elastic.multiprocessing.errors:local_rank 0 FAILED with no error file. Decorate your entrypoint fn with @record for traceback info. See: https://pytorch.org/docs/stable/elastic/errors.html
Traceback (most recent call last):
  File "/localscratch/daril.20493060.0/MYENV/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/localscratch/daril.20493060.0/MYENV/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/localscratch/daril.20493060.0/MYENV/lib/python3.10/site-packages/torch/distributed/run.py", line 794, in main
INFO:torch.distributed.elastic.multiprocessing.errors:local_rank 2 FAILED with no error file. Decorate your entrypoint fn with @record for traceback info. See: https://pytorch.org/docs/stable/elastic/errors.html
    run(args)
  File "/localscratch/daril.20493060.0/MYENV/lib/python3.10/site-packages/torch/distributed/run.py", line 785, in run
Traceback (most recent call last):
  File "/home/daril/.local/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/daril/.local/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    elastic_launch(
  File "/localscratch/daril.20493060.0/MYENV/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/localscratch/daril.20493060.0/MYENV/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
parallelisation_gpu_train_torch_run_multinode.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-08-26_06:13:16
  host      : ng10602.narval.calcul.quebec
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 2496088)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
    return f(*args, **kwargs)
  File "/home/daril/.local/lib/python3.10/site-packages/torch/distributed/run.py", line 794, in main
    run(args)
  File "/home/daril/.local/lib/python3.10/site-packages/torch/distributed/run.py", line 785, in run
INFO:torch.distributed.elastic.multiprocessing.errors:local_rank 1 FAILED with no error file. Decorate your entrypoint fn with @record for traceback info. See: https://pytorch.org/docs/stable/elastic/errors.html
Traceback (most recent call last):
  File "/home/daril/.local/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/daril/.local/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    elastic_launch(
  File "/home/daril/.local/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return f(*args, **kwargs)
  File "/home/daril/.local/lib/python3.10/site-packages/torch/distributed/run.py", line 794, in main
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/daril/.local/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
parallelisation_gpu_train_torch_run_multinode.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-08-26_06:13:21
  host      : ng31103.narval.calcul.quebec
  rank      : 2 (local_rank: 0)
  exitcode  : 1 (pid: 3848166)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
    run(args)
  File "/home/daril/.local/lib/python3.10/site-packages/torch/distributed/run.py", line 785, in run
    elastic_launch(
  File "/home/daril/.local/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/daril/.local/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
parallelisation_gpu_train_torch_run_multinode.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-08-26_06:13:21
  host      : ng30802.narval.calcul.quebec
  rank      : 1 (local_rank: 0)
  exitcode  : 1 (pid: 3927265)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
srun: error: ng30802: task 1: Exited with exit code 1
srun: error: ng31103: task 2: Exited with exit code 1
srun: error: ng10602: task 0: Exited with exit code 1
