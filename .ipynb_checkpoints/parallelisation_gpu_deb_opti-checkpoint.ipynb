{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /home/daril_kw/.venv/lib/python3.10/site-packages (from -r requirements.txt (line 1)) (4.66.1)\n",
      "Requirement already satisfied: torch in /home/daril_kw/.venv/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (2.1.2+cu118)\n",
      "Collecting scikit-learn (from -r requirements.txt (line 3))\n",
      "  Downloading scikit_learn-1.4.1.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting transformers (from -r requirements.txt (line 4))\n",
      "  Downloading transformers-4.37.2-py3-none-any.whl.metadata (129 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.4/129.4 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /home/daril_kw/.venv/lib/python3.10/site-packages (from torch->-r requirements.txt (line 2)) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /home/daril_kw/.venv/lib/python3.10/site-packages (from torch->-r requirements.txt (line 2)) (4.9.0)\n",
      "Requirement already satisfied: sympy in /home/daril_kw/.venv/lib/python3.10/site-packages (from torch->-r requirements.txt (line 2)) (1.12)\n",
      "Requirement already satisfied: networkx in /home/daril_kw/.venv/lib/python3.10/site-packages (from torch->-r requirements.txt (line 2)) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/daril_kw/.venv/lib/python3.10/site-packages (from torch->-r requirements.txt (line 2)) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /home/daril_kw/.venv/lib/python3.10/site-packages (from torch->-r requirements.txt (line 2)) (2023.10.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /home/daril_kw/.venv/lib/python3.10/site-packages (from torch->-r requirements.txt (line 2)) (2.1.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.19.5 in /home/daril_kw/.venv/lib/python3.10/site-packages (from scikit-learn->-r requirements.txt (line 3)) (1.26.2)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn->-r requirements.txt (line 3))\n",
      "  Downloading scipy-1.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.4/60.4 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting joblib>=1.2.0 (from scikit-learn->-r requirements.txt (line 3))\n",
      "  Using cached joblib-1.3.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn->-r requirements.txt (line 3))\n",
      "  Downloading threadpoolctl-3.3.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /home/daril_kw/.venv/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 4)) (0.19.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/daril_kw/.venv/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 4)) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/daril_kw/.venv/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 4)) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/daril_kw/.venv/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 4)) (2023.10.3)\n",
      "Requirement already satisfied: requests in /home/daril_kw/.venv/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 4)) (2.31.0)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers->-r requirements.txt (line 4))\n",
      "  Downloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers->-r requirements.txt (line 4))\n",
      "  Downloading safetensors-0.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/daril_kw/.venv/lib/python3.10/site-packages (from jinja2->torch->-r requirements.txt (line 2)) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/daril_kw/.venv/lib/python3.10/site-packages (from requests->transformers->-r requirements.txt (line 4)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/daril_kw/.venv/lib/python3.10/site-packages (from requests->transformers->-r requirements.txt (line 4)) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/daril_kw/.venv/lib/python3.10/site-packages (from requests->transformers->-r requirements.txt (line 4)) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/daril_kw/.venv/lib/python3.10/site-packages (from requests->transformers->-r requirements.txt (line 4)) (2023.11.17)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/daril_kw/.venv/lib/python3.10/site-packages (from sympy->torch->-r requirements.txt (line 2)) (1.3.0)\n",
      "Downloading scikit_learn-1.4.1.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m54.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.37.2-py3-none-any.whl (8.4 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m56.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "Downloading safetensors-0.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.4 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.4/38.4 MB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.3.0-py3-none-any.whl (17 kB)\n",
      "Downloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m53.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m75.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: threadpoolctl, scipy, safetensors, joblib, scikit-learn, tokenizers, transformers\n",
      "Successfully installed joblib-1.3.2 safetensors-0.4.2 scikit-learn-1.4.1.post1 scipy-1.12.0 threadpoolctl-3.3.0 tokenizers-0.15.2 transformers-4.37.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import datetime\n",
    "import os\n",
    "import pickle\n",
    "import h3\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, matthews_corrcoef\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.optim import AdamW\n",
    "from torch.nn.parallel import DistributedDataParallel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def truncation_rows(df, nb_rows):\n",
    "    return df[:nb_rows]\n",
    "\n",
    "def add_tokenization_column(df, config):\n",
    "    \"\"\"Add a column with the tokenization of the POLYLINE column\n",
    "    /!\\ in that case, for the json file, the trajectories are in the form given by the kaggle dataset /!\\ \n",
    "    /!\\ ie longitude, latitude instead of latitude, longitude.                                        /!\\ \n",
    "    /!\\ This is why we have to reverse the order of the coordinates for the tokenization              /!\\ \"\"\"\n",
    "    \n",
    "    df['Tokenization_2'] = df['POLYLINE'].apply(lambda x: [h3.geo_to_h3(x[i][1], x[i][0], config) for i in range(len(x))])\n",
    "    return df\n",
    "\n",
    "def extract_time_info(df):\n",
    "    \"\"\"Add columns with the day, hour and week of the year knowing the timestamp\"\"\"\n",
    "    df['DATE'] = df['TIMESTAMP'].apply(lambda x: datetime.datetime.fromtimestamp(x).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "    df['DAY'] = df['DATE'].apply(lambda x: str(datetime.datetime.strptime(x.split(' ')[0],'%Y-%m-%d').isocalendar()[2]))\n",
    "    df['HOUR'] = df['DATE'].apply(lambda x: x.split(' ')[1].split(':')[0])\n",
    "    df['WEEK'] = df['DATE'].apply(lambda x: str(datetime.datetime.strptime(x.split(' ')[0],'%Y-%m-%d').isocalendar()[1]))\n",
    "    return df\n",
    "\n",
    "def formatting_to_str(df, column):\n",
    "    \"\"\"Transform the column to string type\"\"\"\n",
    "    if isinstance(df[column][0], str):\n",
    "        return df\n",
    "    df[column] = df[column].astype(str)\n",
    "    return df\n",
    "\n",
    "def call_type_to_nb(df):\n",
    "    \"\"\"Transform the column CALL_TYPE to a number\"\"\"\n",
    "    df['CALL_TYPE'] = df['CALL_TYPE'].apply(lambda x: 0 if x == 'A' else (1 if x == 'B' else 2))\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_geo_and_context_tokens_tokenizer(tokenizer, data_format):\n",
    "    # Add geo tokens to the tokenizer\n",
    "    liste_token_geo = {token for sublist in data_format['Tokenization_2'] for token in sublist}\n",
    "    nb_token_geo = len(liste_token_geo)\n",
    "    tokenizer.add_tokens(list(liste_token_geo))  # Convert set to list and add to tokenizer\n",
    "\n",
    "    # Add contextual info tokens to the tokenizer\n",
    "    contextual_info_token = {str(data_format['CALL_TYPE'][i])\n",
    "                             for i in range(len(data_format))}\n",
    "    contextual_info_token.update(str(data_format['TAXI_ID'][i])\n",
    "                                 for i in range(len(data_format)))\n",
    "    contextual_info_token.update(data_format['DAY'][i]\n",
    "                                 for i in range(len(data_format)))\n",
    "    contextual_info_token.update(data_format['HOUR'][i]\n",
    "                                 for i in range(len(data_format)))\n",
    "    contextual_info_token.update(data_format['WEEK'][i]\n",
    "                                 for i in range(len(data_format)))\n",
    "\n",
    "    tokenizer.add_tokens(list(contextual_info_token))  # Convert set to list and add to tokenizer\n",
    "\n",
    "    return tokenizer, nb_token_geo\n",
    "\n",
    "def add_spaces_for_concat(data_format, column):\n",
    "    \"\"\"Add spaces before and after the values of the column\"\"\" \n",
    "\n",
    "    #We add space before and after the values of the column because we want to separate the tokens (words) with spaces like that : [CLS] 0 1 2 3 4 5 6 7 8 9 10 [SEP]\n",
    "    data_format[column]=data_format[column].apply(lambda x: ' '+x)\n",
    "    return data_format\n",
    "\n",
    "def get_deb_traj(data_format, len_context_info):\n",
    "    \"\"\"the DEB_TRAJ column will be the tokenization column without the last token and the target token\"\"\"\n",
    "    data_format['DEB_TRAJ']=data_format['Tokenization_2'].apply(lambda x: x[:-2])\n",
    "\n",
    "    # we manage the length of the CONTEXT_INPUT column so that after the concatenation, it does not exceed 512 tokens\n",
    "    # the -2 corresponds to the two special tokens [CLS] and [SEP]\n",
    "    # for exemple here, if the trajectory input is too long, we keep the 512-6-2=504 last tokens\n",
    "    data_format['DEB_TRAJ']=data_format['DEB_TRAJ'].apply(lambda x: x[-(512-len_context_info-2):] if len(x)>512-len_context_info-2 else x)    \n",
    "\n",
    "    #then we keep the column in form of a string with spaces between the tokens (the space replaces the comma)\n",
    "    data_format['DEB_TRAJ']=data_format['DEB_TRAJ'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "    return data_format\n",
    "\n",
    "\n",
    "\n",
    "def get_deb_traj_and_target(data_format):\n",
    "    \"\"\"Get the DEB_TRAJ and TARGET columns well formatted but without the special tokens [CLS] and [SEP]\"\"\"\n",
    "\n",
    "    #adding spaces for the concatenation after : we want to sperarate the tokens (words) with spaces\n",
    "    data_format = add_spaces_for_concat(data_format, 'HOUR')\n",
    "    data_format = add_spaces_for_concat(data_format, 'WEEK')\n",
    "    data_format = add_spaces_for_concat(data_format, 'CALL_TYPE')\n",
    "    data_format = add_spaces_for_concat(data_format, 'TAXI_ID')\n",
    "    data_format = add_spaces_for_concat(data_format, 'DAY')\n",
    "\n",
    "    #the column CONTEXT_INPUT will be the concatenation of the last token of the tokenization column + the day + the hour + the week + the call type + the taxi id\n",
    "    data_format['CONTEXT_INPUT'] =data_format['Tokenization_2'].apply(lambda x: x[-1]) + data_format['DAY'] + data_format['HOUR'] + data_format['WEEK'] + data_format['CALL_TYPE'] + data_format['TAXI_ID']\n",
    "    #we get the length of the containing information of the CONTEXT_INPUT column\n",
    "    len_context_info = len(data_format['CONTEXT_INPUT'][0].split(' '))\n",
    "\n",
    "    #we get the DEB_TRAJ column\n",
    "    data_format=get_deb_traj(data_format, len_context_info)\n",
    "    \n",
    "    #we get the TARGET column\n",
    "    data_format['TARGET']=data_format['Tokenization_2'].apply(lambda x: x[-2])\n",
    "\n",
    "    return data_format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def formatting_to_train(data_format, tokenizer):\n",
    "    \"\"\"\n",
    "    Format the data to train the model : \n",
    "    ------------------------------------\n",
    "\n",
    "    1) format the input\n",
    "\n",
    "        a) get the full_inputs\n",
    "    - we concatenate the context input and the beginning of the trajectory which is the sequence we want to give to the model \n",
    "    - at the beginning, we add the CLS token and the end of the input the SEP token\n",
    "\n",
    "        b) get the input_ids\n",
    "    - we use the tokenizer to get the ids of the tokens that will be the input_ids thatthe model will take as input\n",
    "    - we pad the input to the maximum length of 512\n",
    "\n",
    "    2) and we create the attention masks\n",
    "\n",
    "    - the attention mask is a list of 0 and 1, 0 for the padded tokens and 1 for the other tokens\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    #we remove the useless columns\n",
    "    if 'Tokenization' in data_format.columns:\n",
    "        data_format.drop(['Tokenization'],axis=1,inplace=True)\n",
    "    if 'CALL_TYPE' in data_format.columns:\n",
    "        data_format.drop(['CALL_TYPE'],axis=1,inplace=True)\n",
    "    if 'TAXI_ID' in data_format.columns:\n",
    "        data_format.drop(['TAXI_ID'],axis=1,inplace=True)\n",
    "    if 'DAY' in data_format.columns:\n",
    "        data_format.drop(['DAY'],axis=1,inplace=True)\n",
    "    if 'HOUR' in data_format.columns:\n",
    "        data_format.drop(['HOUR'],axis=1,inplace=True)\n",
    "    if 'WEEK' in data_format.columns:\n",
    "        data_format.drop(['WEEK'],axis=1,inplace=True)\n",
    "    if 'Nb_points_token' in data_format.columns:\n",
    "        data_format.drop(['Nb_points_token'],axis=1,inplace=True)\n",
    "\n",
    "\n",
    "    #we get the columns CONTEXT_INPUT, DEB_TRAJ and TARGET\n",
    "    c_inputs=data_format.CONTEXT_INPUT.values\n",
    "    traj_inputs=data_format.DEB_TRAJ.values\n",
    "    targets=data_format.TARGET.values\n",
    "\n",
    "    print(\"concaténation des inputs, padding etc\")\n",
    "\n",
    "    #we create the input_ids, the attention_masks and the full_inputs\n",
    "    input_ids = []\n",
    "    full_inputs = []\n",
    "    attention_masks = []\n",
    "    for i in tqdm(range(len(c_inputs))):\n",
    "        #no truncation is needed because we managed it before\n",
    "\n",
    "        #we concatenate the context input and the trajectory input adding manually the CLS token and the SEP token\n",
    "        full_input = '[CLS] ' + c_inputs[i] + ' ' + traj_inputs[i] + ' [SEP]'\n",
    "        full_inputs.append(full_input)\n",
    "\n",
    "        # we use the tokenizer to get the ids of the tokens that will be the input_ids that the model will take as input\n",
    "        # the format of the input_ids would be : [101] + encoded_c_input + encoded_traj_input + [102]\n",
    "        #the[101] token is the CLS token and the [102] token is the SEP token\n",
    "        # TODO : test adding an additional SEP token between the context input and the trajectory input so that the format of the input_ids would be : [101] + encoded_c_input + [102] + encoded_traj_input + [102]\n",
    "        encoded_full_input=tokenizer.encode(full_input, add_special_tokens=False)\n",
    "\n",
    "        #we pad the input to the maximum length of 512\n",
    "        encoded_full_input=encoded_full_input + [0]*(512-len(encoded_full_input))\n",
    "        #we add the input_ids to the list\n",
    "        input_ids.append(encoded_full_input)\n",
    "\n",
    "        #we create the attention mask\n",
    "        att_mask = [float(i>0) for i in encoded_full_input]\n",
    "        #we add the attention mask to the list\n",
    "        attention_masks.append(att_mask)\n",
    "\n",
    "    return input_ids, attention_masks, targets, full_inputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "WORLD_S=2 #The world size is the number of processes we want to use\n",
    "h3_config_size = 10\n",
    "nb_rows = 60\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we load the data\n",
    "with open('/home/daril_kw/data/02.06.23/train_clean.json', 'r') as openfile:\n",
    "    json_loaded = json.load(openfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_format = pd.DataFrame(data=json_loaded)\n",
    "\n",
    "#we keep only nb_rows rows\n",
    "data_format = truncation_rows(data_format, nb_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we count the number of rows for which teh column NB_POINTS is equal to 0 : there are 0 rows\n",
    "#>>>print(\"nombre de lignes pour lesquelles le nombre de points est inférieur à 3 : \", len(data_format[data_format['Nb_points']<3]))\n",
    "#   nombre de lignes pour lesquelles le nombre de points est inférieur à 3 :  0\n",
    "\n",
    "\n",
    "#we add the tokenization column\n",
    "data_format = add_tokenization_column(data_format, h3_config_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we add the time info columns\n",
    "data_format = extract_time_info(data_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#we remove the useless columns\n",
    "data_format = data_format.drop(['MISSING_DATA','DATE','ORIGIN_CALL', 'DAY_TYPE', 'ORIGIN_CALL', 'ORIGIN_STAND', 'Nb_points', 'TIMESTAMP'], axis=1)\n",
    "\n",
    "#we transform the columns TAXI_ID which was a number to string type\n",
    "data_format = formatting_to_str(data_format, 'TAXI_ID')\n",
    "\n",
    "#we transform the column CALL_TYPE to a number instead of a letter\n",
    "data_format = call_type_to_nb(data_format)\n",
    "\n",
    "#we transform the column CALL_TYPE which was a number to string type\n",
    "data_format = formatting_to_str(data_format, 'CALL_TYPE')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#we remove the useless columns\n",
    "data_format = data_format.drop(['MISSING_DATA','DATE','ORIGIN_CALL', 'DAY_TYPE', 'ORIGIN_CALL', 'ORIGIN_STAND', 'Nb_points', 'TIMESTAMP'], axis=1)\n",
    "\n",
    "#we transform the columns TAXI_ID which was a number to string type\n",
    "data_format = formatting_to_str(data_format, 'TAXI_ID')\n",
    "\n",
    "#we transform the column CALL_TYPE to a number instead of a letter\n",
    "data_format = call_type_to_nb(data_format)\n",
    "\n",
    "#we transform the column CALL_TYPE which was a number to string type\n",
    "data_format = formatting_to_str(data_format, 'CALL_TYPE')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#we get the tokenizer from the HuggingFace library, this one is the tokenizer of the model bert-base-cased but we could have taken the non trained tokenizer (TODO :test it with the non trained weights of the model as well)\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "#we add the geographical and contextual tokens to the tokenizer so that the vocabulary of the tokenizer is adapted to our data\n",
    "tokenizer, nb_token_geo = add_geo_and_context_tokens_tokenizer(tokenizer, data_format)\n",
    "#we get the number of labels which is the number of geographical tokens + 1 (the +1 is for the [SEP] token which is for the end of the sequence and the prediction)\n",
    "nb_labels = nb_token_geo + 1\n",
    "\n",
    "#we get the model from the HuggingFace library, this one is the model bert-base-cased but we could have taken the non trained model (if we want to train it from scratch)\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=nb_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#we add the geographical and contextual tokens to the model so that the size of the model`s embedding is adapted to our data\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "#save the model, the tokenizer and the data in different files\n",
    "model.save_pretrained(f\"/home/daril_kw/data/savings_for_parallel_60/model_before_training_opti_full_for_para_60\")\n",
    "data_format.to_json(f\"/home/daril_kw/data/savings_for_parallel_60/data_with_time_info_ok_opti_full_for_para_60.json\")\n",
    "tokenizer.save_pretrained(f\"/home/daril_kw/data/savings_for_parallel_60/tokenizer_final_opti_full_for_para_60\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#we get the DEB_TRAJ and TARGET columns well formatted but without the special tokens [CLS] and [SEP]\n",
    "#this is because we will add them later\n",
    "data_format = get_deb_traj_and_target(data_format)\n",
    "\n",
    "#we get the input_ids, the attention_masks, the targets and the full_inputs\n",
    "input_ids, attention_masks, targets, full_inputs = formatting_to_train(data_format, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#save the lists full_inputs, inputs_ids, attention_masks and the targets in different files\n",
    "with open(f\"/home/daril_kw/data/savings_for_parallel_60/input_ids_full_opti_for_para_60.pkl\", 'wb') as fp:\n",
    "    pickle.dump(input_ids, fp)\n",
    "with open(f\"/home/daril_kw/data/savings_for_parallel_60/attention_masks_full_opti_for_para_60.pkl\", 'wb') as fp:\n",
    "    pickle.dump(attention_masks, fp)\n",
    "with open(f\"/home/daril_kw/data/savings_for_parallel_60/targets_full_opti_for_para_60.pkl\", 'wb') as fp:\n",
    "    pickle.dump(targets, fp)\n",
    "with open(f\"/home/daril_kw/data/savings_for_parallel_60/full_inputs_full_opti_for_para_60.pkl\", 'wb') as fp:\n",
    "    pickle.dump(full_inputs, fp)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
