{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "import json\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader, SequentialSampler\n",
    "import argparse\n",
    "\n",
    "\n",
    "# DIR_INPUTS_IDS = (\n",
    "#     \"/home/daril/trajcbert/savings_for_parrallel_1_2/input_ids_f_833383.pkl\"\n",
    "# )\n",
    "# DIR_ATTENTION_MASKS = (\n",
    "#     \"/home/daril/trajcbert/savings_for_parrallel_1_2/attention_masks_833383_opti.pkl\"\n",
    "# )\n",
    "# DIR_TARGETS = \"/home/daril/trajcbert/savings_for_parrallel_1_2/targets_833383_opti.pkl\"\n",
    "# PRETRAINED_MODEL_NAME = (\n",
    "#     \"/home/daril/trajcbert/savings_for_parrallel_1_2/model_before_training_opti_833383\"\n",
    "# )\n",
    "# DATALOADER_DIR = \"/home/daril/trajcbert/savings/test_dataloader_833383.pt\"\n",
    "\n",
    "\n",
    "def save_test_data_loader(\n",
    "    input_ids_path,\n",
    "    attention_masks_path,\n",
    "    targets_path,\n",
    "    dataloader_dir,\n",
    "    batch_size=32,\n",
    "):\n",
    "    # load the lists saved in deb_train_gpu_parallel.py\n",
    "    # the lists saved full_inputs, inputs_ids, attention_masks and the targets in different files /home/daril_kw/data/input_ids.pkl, /home/daril_kw/data/attention_masks.pkl, /home/daril_kw/data/targets.pkl\n",
    "\n",
    "    with open(input_ids_path, \"rb\") as f:\n",
    "        input_ids = pickle.load(f)\n",
    "\n",
    "    with open(attention_masks_path, \"rb\") as f:\n",
    "        attention_masks = pickle.load(f)\n",
    "\n",
    "    with open(targets_path, \"rb\") as f:\n",
    "        targets = pickle.load(f)\n",
    "\n",
    "    targets_dict = {}\n",
    "    # create a dictionary to convert the targets to numbers\n",
    "    for i in range(len(targets)):\n",
    "        if targets[i] not in targets_dict:\n",
    "            targets_dict[targets[i]] = len(targets_dict)\n",
    "\n",
    "    targets_input = [targets_dict[targets[i]] for i in range(len(targets))]\n",
    "\n",
    "    train_data, test_data, train_targets, test_targets = train_test_split(\n",
    "        input_ids, targets_input, random_state=2023, test_size=0.2\n",
    "    )\n",
    "\n",
    "    # the two _ are for test data and test targets\n",
    "\n",
    "    train_masks, test_mask, _, _ = train_test_split(\n",
    "        attention_masks, targets_input, random_state=2023, test_size=0.2\n",
    "    )\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # we create the dataloader for the test data\n",
    "\n",
    "    test_inputs = torch.tensor(test_data)\n",
    "    test_labels = torch.tensor(test_targets)\n",
    "    test_masks = torch.tensor(test_mask)\n",
    "    test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "    test_sampler = SequentialSampler(\n",
    "        test_data\n",
    "    )  # we don't use the DistributedSampler here because the validation is on a CPU\n",
    "    test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n",
    "    # save the test dataloader\n",
    "\n",
    "    torch.save(test_dataloader, dataloader_dir)\n",
    "\n",
    "    return test_dataloader\n",
    "\n",
    "\n",
    "def main():\n",
    "    # recover the parameters\n",
    "    # --inputs_ids_path $DIR_INPUTS_IDS \\\n",
    "    # --attention_masks_path $DIR_ATTENTION_MASKS \\\n",
    "    # --targets_path $DIR_TARGETS \\\n",
    "    # --dataloader_dir $DATALOADER_DIR\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--inputs_ids_path\", type=str, help=\"path to the inputs ids\")\n",
    "    parser.add_argument(\n",
    "        \"--attention_masks_path\", type=str, help=\"path to the attention masks\"\n",
    "    )\n",
    "    parser.add_argument(\"--targets_path\", type=str, help=\"path to the targets\")\n",
    "    parser.add_argument(\"--dataloader_dir\", type=str, help=\"path to the dataloader\")\n",
    "    args = parser.parse_args()\n",
    "    DIR_INPUTS_IDS = args.inputs_ids_path\n",
    "    DIR_ATTENTION_MASKS = args.attention_masks_path\n",
    "    DIR_TARGETS = args.targets_path\n",
    "    DATALOADER_DIR = args.dataloader_dir\n",
    "\n",
    "    save_test_data_loader(\n",
    "        DIR_INPUTS_IDS,\n",
    "        DIR_ATTENTION_MASKS,\n",
    "        DIR_TARGETS,\n",
    "        DATALOADER_DIR,\n",
    "        batch_size=32,\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working 0\n",
      "working 1\n",
      "working 2\n",
      "working 3\n",
      "working 4\n",
      "working 5\n",
      "working 6\n",
      "working 7\n",
      "working 8\n",
      "working 9\n",
      "working 10\n",
      "working 11\n",
      "working 12\n",
      "working 13\n",
      "working 14\n",
      "working 15\n",
      "working 16\n",
      "working 17\n",
      "working 18\n",
      "working 19\n",
      "working 20\n",
      "working 21\n",
      "working 22\n",
      "working 23\n",
      "working 24\n",
      "working 25\n",
      "working 26\n",
      "working 27\n",
      "working 28\n",
      "working 29\n"
     ]
    }
   ],
   "source": [
    "def test_working(): \n",
    "    for i in range (30):\n",
    "        print(f\"working in browser {i}\")\n",
    "        # slepp for 1 second\n",
    "        time.sleep(1)\n",
    "\n",
    "test_working()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working 0\n",
      "working 1\n",
      "working 2\n",
      "working 3\n",
      "working 4\n",
      "working 5\n",
      "working 6\n",
      "working 7\n",
      "working 8\n",
      "working 9\n",
      "working 10\n",
      "working 11\n",
      "working 12\n",
      "working 13\n",
      "working 14\n",
      "working 15\n",
      "working 16\n",
      "working 17\n",
      "working 18\n",
      "working 19\n",
      "working 20\n",
      "working 21\n",
      "working 22\n",
      "working 23\n",
      "working 24\n",
      "working 25\n",
      "working 26\n",
      "working 27\n",
      "working 28\n",
      "working 29\n"
     ]
    }
   ],
   "source": [
    "test_working()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
